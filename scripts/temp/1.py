# verify_bart_load.py
import os
import logging
import asyncio
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

# ******** å¼ºåˆ¶ Hugging Face Hub è¿›å…¥ç¦»çº¿æ¨¡å¼ ********
# è¿™ç¡®ä¿äº†æ¨¡å‹ä¼šä»æœ¬åœ°ç¼“å­˜åŠ è½½ï¼Œè€Œä¸æ˜¯å°è¯•ä»ç½‘ç»œä¸‹è½½ã€‚
os.environ["HF_HUB_OFFLINE"] = "1"
# ******************************************************

# å¯¼å…¥ settings å¯¹è±¡ï¼Œä»¥è·å–æ¨¡å‹è·¯å¾„
try:
    from config.settings import settings
except ImportError:
    logging.critical("âŒ é”™è¯¯ï¼šæ— æ³•å¯¼å…¥ config.settingsã€‚è¯·ç¡®ä¿ config/settings.py æ–‡ä»¶å­˜åœ¨ä¸”å¯è®¿é—®ã€‚")
    exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def verify_bart_model_load():
    """
    å¼‚æ­¥éªŒè¯ BART æ‘˜è¦æ¨¡å‹åœ¨ç¦»çº¿æ¨¡å¼ä¸‹æ˜¯å¦èƒ½ä»æœ¬åœ°è·¯å¾„åŠ è½½ã€‚
    """
    model_path = settings.SUMMARY_MODEL_PATH
    model_name = settings.SUMMARY_MODEL_HUB_NAME # ç”¨äºæ—¥å¿—æ˜¾ç¤º

    logger.info(f"éªŒè¯è„šæœ¬å¯åŠ¨ã€‚å½“å‰ HF_HUB_OFFLINE ç¯å¢ƒå˜é‡: {os.environ.get('HF_HUB_OFFLINE')}")
    
    # æ£€æµ‹å¯ç”¨è®¾å¤‡
    device_str = "cuda" if torch.cuda.is_available() else "cpu"
    device = torch.device(device_str)
    logger.info(f"æ£€æµ‹åˆ°çš„è®¾å¤‡: {device_str.upper()}")

    model_loaded_successfully = False

    # 1. æ£€æŸ¥æœ¬åœ°æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        logger.critical(f"âŒ é”™è¯¯ï¼šæ‘˜è¦æ¨¡å‹æœ¬åœ°è·¯å¾„ä¸å­˜åœ¨: '{model_path}'ã€‚è¯·ç¡®ä¿æ–‡ä»¶å·²æ­£ç¡®æ”¾ç½®ã€‚")
        logger.critical("è¯·å†æ¬¡ç¡®è®¤æ‚¨æ‰‹åŠ¨ä¸‹è½½çš„æ–‡ä»¶æ˜¯å¦çœŸçš„åœ¨ä¸Šè¿°è·¯å¾„ä¸‹ï¼Œå¹¶ä¸”æ–‡ä»¶å¤¹ç»“æ„æ­£ç¡®ã€‚")
        return False

    logger.info(f"æœ¬åœ°æ¨¡å‹è·¯å¾„ '{model_path}' å­˜åœ¨ã€‚")

    # 2. å°è¯•åŠ è½½åˆ†è¯å™¨
    tokenizer = None
    try:
        logger.info(f"å°è¯•ä»æœ¬åœ°è·¯å¾„åŠ è½½åˆ†è¯å™¨: '{model_path}'...")
        tokenizer = await asyncio.to_thread(
            AutoTokenizer.from_pretrained,
            model_path, # ç›´æ¥ä½¿ç”¨æœ¬åœ°è·¯å¾„
            local_files_only=True # æ˜ç¡®æŒ‡å®šåªä»æœ¬åœ°æ–‡ä»¶åŠ è½½
        )
        logger.info("âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸã€‚")
    except Exception as e:
        logger.critical(f"âŒ é”™è¯¯ï¼šåˆ†è¯å™¨åŠ è½½å¤±è´¥ï¼è¯·æ£€æŸ¥ '{model_path}' ä¸­çš„åˆ†è¯å™¨æ–‡ä»¶æ˜¯å¦å®Œæ•´ä¸”å…¼å®¹ã€‚")
        logger.critical(f"é”™è¯¯ä¿¡æ¯ (åˆ†è¯å™¨): {e}", exc_info=True)
        return False

    # 3. å°è¯•åŠ è½½æ¨¡å‹
    model = None
    try:
        logger.info(f"å°è¯•ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹: '{model_path}' (è®¾å¤‡: {device_str})...")
        model = await asyncio.to_thread(
            AutoModelForSeq2SeqLM.from_pretrained,
            model_path, # ç›´æ¥ä½¿ç”¨æœ¬åœ°è·¯å¾„
            local_files_only=True # æ˜ç¡®æŒ‡å®šåªä»æœ¬åœ°æ–‡ä»¶åŠ è½½
        )
        model.to(device) # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸã€‚")
    except Exception as e:
        logger.critical(f"âŒ é”™è¯¯ï¼šæ¨¡å‹åŠ è½½å¤±è´¥ï¼è¯·æ£€æŸ¥ '{model_path}' ä¸­çš„æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´ä¸”å…¼å®¹ã€‚")
        logger.critical(f"é”™è¯¯ä¿¡æ¯ (æ¨¡å‹): {e}", exc_info=True)
        return False

    # 4. åˆ›å»º Hugging Face pipeline
    pipeline_instance = None
    try:
        logger.info("æ­£åœ¨åˆ›å»º Hugging Face pipeline...")
        pipeline_instance = await asyncio.to_thread(
            pipeline,
            "summarization", # ä»»åŠ¡ç±»å‹
            model=model,
            tokenizer=tokenizer,
            device=0 if device_str == "cuda" else -1 # 0 for GPU, -1 for CPU
        )
        logger.info("âœ… Hugging Face pipeline åˆ›å»ºæˆåŠŸã€‚")
    except Exception as e:
        logger.critical(f"âŒ é”™è¯¯ï¼šåˆ›å»º Hugging Face pipeline å¤±è´¥ï¼")
        logger.critical(f"é”™è¯¯ä¿¡æ¯ (pipeline): {e}", exc_info=True)
        return False

    model_loaded_successfully = True
    logger.info(f"ğŸ‰ æ‘˜è¦æ¨¡å‹ '{model_name}' å·²æˆåŠŸä»æœ¬åœ°è·¯å¾„åŠ è½½åˆ° {device_str.upper()}ã€‚")

    # 5. è¿›è¡Œä¸€ä¸ªç®€å•çš„æ‘˜è¦æµ‹è¯•
    test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºéªŒè¯æ‘˜è¦æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚å®ƒåº”è¯¥èƒ½å¤Ÿç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„æ€»ç»“ã€‚"
    logger.info(f"\næ­£åœ¨å¯¹æµ‹è¯•æ–‡æœ¬è¿›è¡Œæ‘˜è¦: '{test_text}'")
    try:
        summary_list = await asyncio.to_thread(
            pipeline_instance,
            test_text,
            max_length=50,
            min_length=10,
            num_beams=4,
            do_sample=False
        )
        summary_text = summary_list[0]['summary_text']
        logger.info(f"âœ… æ‘˜è¦ç”Ÿæˆæµ‹è¯•æˆåŠŸã€‚ç”Ÿæˆçš„æ‘˜è¦: '{summary_text}'")
    except Exception as e:
        logger.critical(f"âŒ é”™è¯¯ï¼šæ‘˜è¦ç”Ÿæˆæµ‹è¯•å¤±è´¥ï¼å³ä½¿æ¨¡å‹åŠ è½½æˆåŠŸï¼Œæ¨ç†ä¹Ÿå¯èƒ½å‡ºç°é—®é¢˜ã€‚")
        logger.critical(f"é”™è¯¯ä¿¡æ¯ (æ‘˜è¦æµ‹è¯•): {e}", exc_info=True)
        model_loaded_successfully = False # æ ‡è®°ä¸ºå¤±è´¥ï¼Œå› ä¸ºæ¨ç†ä¸å·¥ä½œ

    return model_loaded_successfully

if __name__ == "__main__":
    if asyncio.run(verify_bart_model_load()):
        logger.info("\nğŸ‰ æ‘˜è¦æ¨¡å‹å·²å®Œå…¨éªŒè¯æˆåŠŸã€‚")
    else:
        logger.critical("\nâŒ æ‘˜è¦æ¨¡å‹éªŒè¯å¤±è´¥ã€‚è¯·æ£€æŸ¥ä¸Šè¿°æ—¥å¿—ä»¥è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯ã€‚")

