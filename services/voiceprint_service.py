# M:\meeting\services\voiceprint_service.py

import os
import torch
import logging
from typing import Dict, Any, Optional, List, Tuple
import numpy as np
import asyncio
import uuid
from dotenv import load_dotenv 
import time
from datetime import datetime

# å¯¼å…¥é…ç½®
from config.settings import settings
from services.milvus_service import MilvusManager
from services.mongodb_manager import MongoDBManager
from models import User, UserRole
from core.speech_to_text.stt_processor import SpeechToTextProcessor 

load_dotenv()
logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥ pyannote.audio
try:
    from pyannote.audio import Pipeline
    from pyannote.audio.core.model import Model
    from pyannote.core import Segment, Annotation
    from pyannote.audio import Audio
    import torchaudio
    logger.info("pyannote.audio å’Œ torchaudio å¯¼å…¥æˆåŠŸã€‚")
except ImportError:
    logger.warning("æ— æ³•å¯¼å…¥ pyannote.audio æˆ– torchaudioã€‚å£°çº¹è¯†åˆ«å’Œè¯´è¯äººåˆ†ç¦»åŠŸèƒ½å°†å—é™ã€‚")
    Pipeline = None
    Model = None
    Audio = None
    torchaudio = None

class VoiceprintService:
    """
    VoiceprintService è´Ÿè´£å£°çº¹çš„æ³¨å†Œã€è¯†åˆ«å’Œè¯´è¯äººåˆ†ç¦»ã€‚
    æ­¤ç‰ˆæœ¬å®ç°äº†å»¶è¿Ÿæ³¨å†Œç­–ç•¥ï¼š
    - çŸ­éŸ³é¢‘ç‰‡æ®µï¼ˆä¸è¶³ä»¥æå–å¯é å£°çº¹ï¼‰å°†è¢«ä¸´æ—¶å­˜å‚¨ã€‚
    - å½“ä¸€ä¸ªè¶³å¤Ÿé•¿çš„éŸ³é¢‘ç‰‡æ®µå‡ºç°æ—¶ï¼Œå°†å°è¯•æ³¨å†Œæ–°å£°çº¹ï¼Œå¹¶å°†ä¹‹å‰çš„ä¸´æ—¶ç‰‡æ®µåˆå¹¶åˆ°æ–°ç”¨æˆ·IDä¸‹ã€‚
    """
    def __init__(self, settings_obj: settings, voice_milvus_manager: MilvusManager, mongodb_manager: MongoDBManager):
        logger.info("åˆå§‹åŒ– VoiceprintService...")
        self.settings = settings_obj

        self.device_str = "cuda" if self.settings.USE_CUDA and torch.cuda.is_available() else "cpu"
        self.device = torch.device(self.device_str)
        logger.info(f"æ£€æµ‹åˆ°çš„è®¾å¤‡: {self.device_str.upper()}")
        self.min_segment_duration = getattr(self.settings, 'MIN_AUDIO_SEGMENT_DURATION_S', 0.5)
        # è¿™é‡Œçš„ MIN_AUDIO_SAMPLES å’Œ VOICE_EMBEDDING_MIN_DURATION éƒ½ç”¨äºå®šä¹‰æœ€å°å£°çº¹æ—¶é•¿
        self.MIN_AUDIO_SAMPLES = int(self.settings.MIN_SPEECH_SEGMENT_DURATION * self.settings.VOICE_SAMPLE_RATE)
        logger.info(f"å£°çº¹åµŒå…¥æœ€å°éŸ³é¢‘é•¿åº¦è®¾ç½®ä¸º {self.MIN_AUDIO_SAMPLES} é‡‡æ ·ç‚¹ ({self.MIN_AUDIO_SAMPLES / self.settings.VOICE_SAMPLE_RATE:.1f} ç§’)ã€‚")

        self.diarization_pipeline: Optional[Pipeline] = None
        self.embedding_model: Optional[Model] = None 
        self.audio_processor: Optional[Audio] = None
        self._model_loaded: bool = False
        self.voice_milvus_manager = voice_milvus_manager
        self.mongodb_manager = mongodb_manager
        self.registered_voiceprints_cache: Dict[str, Dict[str, Any]] = {}
        
        self.realtime_buffers: Dict[str, List[np.ndarray]] = {} 
        self.buffer_start_time: Dict[str, float] = {} 
        
        self.min_speech_off_duration = self.settings.MIN_SPEECH_DURATION_OFF

        # --- æ–°å¢ï¼šç”¨äºVADé©±åŠ¨çš„å®æ—¶éŸ³é¢‘ç¼“å†² ---
        # é”®ä¸º meeting_idï¼Œå€¼ä¸ºç´¯ç§¯çš„éŸ³é¢‘æ•°æ®å—åˆ—è¡¨
        self.speech_buffer: Dict[str, List[np.ndarray]] = {}
        # é”®ä¸º meeting_idï¼Œå€¼ä¸ºæœ€åä¸€æ¬¡æ£€æµ‹åˆ°è¯­éŸ³æ´»åŠ¨çš„æ—¶é—´æˆ³
        self.last_speech_timestamp: Dict[str, float] = {}

        logger.info("VoiceprintService åˆå§‹åŒ–å®Œæˆï¼Œæ¨¡å‹å¾…å¼‚æ­¥åŠ è½½ã€‚")

    async def load_model(self):
        """
        å¼‚æ­¥åŠ è½½è¯´è¯äººåˆ†ç¦»å’ŒåµŒå…¥æ¨¡å‹ã€‚
        """
        if self._model_loaded:
            logger.info("VoiceprintService æ¨¡å‹å·²åŠ è½½ï¼Œè·³è¿‡é‡å¤åŠ è½½ã€‚")
            return
        
        try:
            os.environ["HF_HUB_OFFLINE"] = self.settings.HF_HUB_OFFLINE
            
            hf_token = None
            if self.settings.HF_TOKEN:
                if isinstance(self.settings.HF_TOKEN, str):
                    hf_token = self.settings.HF_TOKEN
                elif hasattr(self.settings.HF_TOKEN, 'get_secret_value'):
                    hf_token = self.settings.HF_TOKEN.get_secret_value()
            
            if not hf_token and self.settings.HF_HUB_OFFLINE == "0":
                logger.warning("HF_TOKEN æœªè®¾ç½®ï¼Œåœ¨çº¿æ¨¡å¼ä¸‹å¯èƒ½æ— æ³•ä¸‹è½½ pyannote.audio æ¨¡å‹ã€‚")

            if Pipeline:
                try:
                    self.diarization_pipeline = Pipeline.from_pretrained(
                        self.settings.PYANNOTE_DIARIZATION_MODEL,
                        use_auth_token=hf_token
                    )
                    if self.diarization_pipeline:
                        self.diarization_pipeline.to(self.device)
                        logger.info("âœ… Pyannote Diarization Pipeline åŠ è½½æˆåŠŸã€‚")
                except Exception as e:
                    logger.error(f"åŠ è½½ Pyannote Diarization Pipeline å¤±è´¥: {e}")

            # --- æ–°å¢: åŠ è½½ Silero VAD æ¨¡å‹ ---
            try:
                vad_model_path = self.settings.VAD_MODEL_PATH
                if vad_model_path.exists():
                    self.vad_model, self.vad_utils = torch.hub.load(
                        repo_or_dir=str(self.settings.BASE_DIR / "models" / "silero_vad"),
                        model='silero_vad',
                        source='local',
                        onnx=False
                    )
                    self.vad_model.to(self.device)
                    logger.info(f"âœ… Silero VAD æ¨¡å‹åŠ è½½æˆåŠŸ: {vad_model_path}")
                else:
                    logger.warning(f"âš ï¸ VAD æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {vad_model_path}ï¼Œå°†å›é€€åˆ°åŸºç¡€èƒ½é‡æ£€æµ‹ã€‚")
                    self.vad_model = None
            except Exception as e:
                logger.error(f"åŠ è½½ Silero VAD å¤±è´¥: {e}")
                self.vad_model = None

            if Model:
                logger.info(f"å°è¯•åŠ è½½è¯´è¯äººåµŒå…¥æ¨¡å‹: '{self.settings.PYANNOTE_EMBEDDING_MODEL}'...")
                self.embedding_model = await asyncio.to_thread(
                    Model.from_pretrained,
                    self.settings.PYANNOTE_EMBEDDING_MODEL,
                    use_auth_token=hf_token,
                    strict=False,
                )
                self.embedding_model.to(self.device)
                logger.info("ğŸ‰ è¯´è¯äººåµŒå…¥æ¨¡å‹å·²æˆåŠŸåŠ è½½ã€‚")
            else:
                logger.warning("pyannote.audio.core.model.Model æœªå¯¼å…¥ï¼Œè¯´è¯äººåµŒå…¥æ¨¡å‹æ— æ³•åŠ è½½ã€‚")
                raise RuntimeError("pyannote.audio.core.model.Model æœªå¯¼å…¥ï¼ŒæœåŠ¡æ— æ³•å¯åŠ¨ã€‚")

            if Audio:
                self.audio_processor = Audio(sample_rate=self.settings.VOICE_SAMPLE_RATE)
                logger.info(f"Audio å¤„ç†å™¨é‡‡æ ·ç‡è®¾ç½®ä¸º: {self.audio_processor.sample_rate} Hzã€‚")
            else:
                logger.warning("pyannote.audio.Audio æœªå¯¼å…¥ï¼ŒéŸ³é¢‘å¤„ç†å™¨æ— æ³•åˆå§‹åŒ–ã€‚")
                raise RuntimeError("pyannote.audio.Audio æœªå¯¼å…¥ï¼ŒæœåŠ¡æ— æ³•å¯åŠ¨ã€‚")

            await self._load_registered_voiceprints_from_milvus()

            self._model_loaded = True
            logger.info("VoiceprintService æ¨¡å‹åŠ è½½å®Œæˆã€‚")
        except Exception as e:
            logger.critical(f"âŒ é”™è¯¯ï¼šVoiceprintService æ¨¡å‹åŠ è½½å¤±è´¥ï¼")
            logger.critical(f"é”™è¯¯ä¿¡æ¯: {e}", exc_info=True)
            self.diarization_pipeline = None
            self.embedding_model = None
            self.audio_processor = None
            self._model_loaded = False
            raise RuntimeError(f"VoiceprintService åˆå§‹åŒ–å¤±è´¥: {str(e)}") from e

    def is_model_loaded(self) -> bool:
        """
        æ£€æŸ¥æ‰€æœ‰æ¨¡å‹æ˜¯å¦å·²åŠ è½½ã€‚
        """
        return self._model_loaded

    async def _load_registered_voiceprints_from_milvus(self):
        """
        ä» Milvus åŠ è½½æ‰€æœ‰å·²æ³¨å†Œçš„å£°çº¹å…ƒæ•°æ®åˆ°æœ¬åœ°ç¼“å­˜ã€‚
        ç¼“å­˜ç»“æ„ä¸º { user_id: { "embedding": np.ndarray, "username": str, "role": str } }
        """
        logger.info("æ­£åœ¨ä» Milvus åŠ è½½å·²æ³¨å†Œå£°çº¹åˆ°æœ¬åœ°ç¼“å­˜...")
        if not self.voice_milvus_manager or not self.voice_milvus_manager.is_connected:
            logger.error("MilvusManager æœªè¿æ¥æˆ–æœªåˆå§‹åŒ–ï¼Œæ— æ³•åŠ è½½å·²æ³¨å†Œå£°çº¹ã€‚")
            self.registered_voiceprints_cache = {}
            return

        try:
            milvus_data = await self.voice_milvus_manager.get_all_data(output_fields=["id", "user_name", "role", "embedding"])
            
            self.registered_voiceprints_cache = {}
            for entry in milvus_data:
                user_id = entry.get("id")
                embedding = entry.get("embedding")
                user_name = entry.get("user_name")
                role = entry.get("role")

                if user_id and embedding and user_name and role:
                    self.registered_voiceprints_cache[user_id] = {
                        "embedding": np.array(embedding, dtype=np.float32),
                        "username": user_name,
                        "role": role
                    }
                    logger.debug(f"å·²åŠ è½½å£°çº¹: {user_name} ({user_id})")
                else:
                    logger.warning(f"ä» Milvus åŠ è½½å£°çº¹æ—¶å‘ç°ä¸å®Œæ•´æ•°æ®: {entry}")

            logger.info(f"æˆåŠŸä» Milvus åŠ è½½ {len(self.registered_voiceprints_cache)} æ¡å£°çº¹åˆ°ç¼“å­˜ã€‚")
        except Exception as e:
            logger.error(f"ä» Milvus åŠ è½½å·²æ³¨å†Œå£°çº¹åˆ°ç¼“å­˜å¤±è´¥: {e}", exc_info=True)
            self.registered_voiceprints_cache = {}

    async def _get_embedding(self, audio_data: np.ndarray, sample_rate: int) -> Optional[List[float]]:
        """
        ä»éŸ³é¢‘æ•°æ®ä¸­æå–å•ä¸ªè¯´è¯äººçš„å£°çº¹åµŒå…¥ã€‚
        """
        if not self.embedding_model or not self.audio_processor or not torchaudio:
            logger.error("å£°çº¹åµŒå…¥æ¨¡å‹ã€éŸ³é¢‘å¤„ç†å™¨æˆ– torchaudio æœªåŠ è½½ï¼Œæ— æ³•æå–å£°çº¹ã€‚")
            return None

        # æœ€å°é‡‡æ ·ç‚¹æ•°ï¼Œç”¨äºæå–å£°çº¹
        min_samples_for_embedding = int(self.settings.VOICE_EMBEDDING_MIN_DURATION * self.settings.VOICE_SAMPLE_RATE)

        if audio_data.shape[-1] < min_samples_for_embedding:
            logger.warning(f"éŸ³é¢‘ç‰‡æ®µå¤ªçŸ­ ({audio_data.shape[-1]} é‡‡æ ·ç‚¹)ï¼Œä¸è¶³ä»¥æå–å£°çº¹ã€‚æœ€å°è¦æ±‚: {min_samples_for_embedding} é‡‡æ ·ç‚¹ã€‚")
            return None
        
        if np.isnan(audio_data).any() or np.isinf(audio_data).any():
            logger.error("è¾“å…¥éŸ³é¢‘æ•°æ®åŒ…å« NaN æˆ– Inf å€¼ï¼Œæ— æ³•è¿›è¡Œå£°çº¹æå–ã€‚")
            return None

        try:
            waveform = torch.from_numpy(audio_data).float().to(self.device)
            
            if sample_rate != self.settings.VOICE_SAMPLE_RATE:
                logger.debug(f"é‡é‡‡æ ·éŸ³é¢‘ä» {sample_rate} Hz åˆ° {self.settings.VOICE_SAMPLE_RATE} Hzã€‚")
                resampler = torchaudio.transforms.Resample(sample_rate, self.settings.VOICE_SAMPLE_RATE).to(self.device)
                waveform = resampler(waveform)
            
            if waveform.ndim == 1:
                waveform = waveform.unsqueeze(0).unsqueeze(0)
            elif waveform.ndim == 2:
                if waveform.shape[0] == 1:
                    waveform = waveform.unsqueeze(1)
                elif waveform.shape[1] == 1:
                    waveform = waveform.permute(1, 0).unsqueeze(0)
                else:
                    waveform = waveform.unsqueeze(0)
                
            with torch.no_grad():
                embedding = await asyncio.to_thread(self.embedding_model, waveform)
            
            return embedding.cpu().detach().numpy().squeeze().tolist()

        except Exception as e:
            logger.error(f"æå–å£°çº¹åµŒå…¥å¤±è´¥: {e}", exc_info=True)
            return None

    async def register_voice(self, audio_data: np.ndarray, sample_rate: int, user_id: str, username: str, role: str) -> Dict[str, Any]:
        """
        æ³¨å†Œç”¨æˆ·çš„å£°çº¹ã€‚
        """
        logger.info(f"VoiceprintService: å°è¯•æ³¨å†Œå£°çº¹ for user_id: {user_id}, username: {username}, role: {role}")
        if not self.voice_milvus_manager or not self.voice_milvus_manager.is_connected:
            raise RuntimeError("MilvusManager æœªåˆå§‹åŒ–æˆ–æœªè¿æ¥ã€‚æ— æ³•æ³¨å†Œå£°çº¹ã€‚")
        if not self.mongodb_manager:
            raise RuntimeError("MongoDBManager æœªåˆå§‹åŒ–ã€‚æ— æ³•æ³¨å†Œå£°çº¹ã€‚")

        embedding = await self._get_embedding(audio_data, sample_rate)
        if embedding is None:
            raise ValueError("æ— æ³•ä»æä¾›çš„éŸ³é¢‘ç”Ÿæˆå£°çº¹åµŒå…¥ã€‚è¯·ç¡®ä¿éŸ³é¢‘è´¨é‡å’Œæ—¶é•¿ç¬¦åˆè¦æ±‚ã€‚")

        try:
            milvus_data_entry = {
                "id": user_id,
                "user_name": username,
                "role": role,
                "embedding": embedding
            }
            
            pks = await self.voice_milvus_manager.insert_data([milvus_data_entry])
            
            if pks:
                self.registered_voiceprints_cache[user_id] = {
                    "embedding": np.array(embedding, dtype=np.float32),
                    "username": username,
                    "role": role
                }
                logger.info(f"å£°çº¹ for user_id: {user_id} å·²æˆåŠŸæ³¨å†Œåˆ° Milvus å¹¶ç¼“å­˜ã€‚")

                from models import User
                user_obj = User(user_id=user_id, username=username, role=UserRole(role.upper()))
                await self.mongodb_manager.add_or_update_user(user_obj)
                logger.info(f"ç”¨æˆ· '{username}' (ID: {user_id}) çš„å…ƒæ•°æ®å·²ä¿å­˜/æ›´æ–°åˆ° MongoDBã€‚")

                return {"status": "registered", "user_id": user_id, "is_new_user": True, "message": "å£°çº¹æ³¨å†ŒæˆåŠŸ"}
            else:
                raise RuntimeError("Milvus æ’å…¥æ“ä½œæœªè¿”å› IDã€‚")
        except Exception as e:
            logger.error(f"æ³¨å†Œå£°çº¹å¤±è´¥ for user_id: {user_id}: {e}", exc_info=True)
            raise RuntimeError(f"å£°çº¹æ³¨å†Œå¤±è´¥: {str(e)}")

    async def identify_speaker(self, audio_data: np.ndarray, sample_rate: int) -> Dict[str, Any]:
        """
        è¯†åˆ«éŸ³é¢‘ä¸­çš„è¯´è¯äººã€‚
        """
        logger.debug("VoiceprintService: å°è¯•è¯†åˆ«è¯´è¯äºº...")
        
        default_unknown_user = {"user_id": None, "username": "æœªçŸ¥ç”¨æˆ·", "role": UserRole.UNKNOWN.value, "is_known": False, "confidence": 0}

        if not self.voice_milvus_manager or not self.voice_milvus_manager.is_connected:
            logger.error("MilvusManager æœªåˆå§‹åŒ–æˆ–æœªè¿æ¥ã€‚æ— æ³•è¯†åˆ«è¯´è¯äººã€‚")
            return default_unknown_user
        
        if not self.registered_voiceprints_cache:
            logger.warning("æ²¡æœ‰å·²æ³¨å†Œçš„å£°çº¹ï¼Œæ— æ³•è¿›è¡Œè¯†åˆ«ã€‚å°†è¿”å›æœªçŸ¥ç”¨æˆ·ã€‚")
            return default_unknown_user

        query_embedding = await self._get_embedding(audio_data, sample_rate)
        if query_embedding is None:
            logger.error("æ— æ³•ä»æŸ¥è¯¢éŸ³é¢‘ä¸­æå–åµŒå…¥å‘é‡ã€‚å°†è¿”å›æœªçŸ¥ç”¨æˆ·ã€‚")
            return default_unknown_user

        try:
            search_results = await self.voice_milvus_manager.search_data(
                query_vectors=[query_embedding], 
                top_k=1,
                output_fields=["user_name", "role"]
            )

            if search_results and search_results[0]: # ç¡®ä¿æœ‰ç»“æœä¸”ç¬¬ä¸€ä¸ªç»“æœåˆ—è¡¨ä¸ä¸ºç©º
                # Milvus search_data è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ª HybridHits å¯¹è±¡ã€‚
                # æˆ‘ä»¬éœ€è¦è·å–ç¬¬ä¸€ä¸ªæŸ¥è¯¢ç»“æœï¼ˆsearch_results[0]ï¼‰ï¼Œå†è·å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„ Hit å¯¹è±¡ã€‚
                best_match = search_results[0][0]
                
                # --- ä¿®å¤ç‚¹ï¼šç›´æ¥é€šè¿‡å±æ€§è®¿é—® Milvus Hit å¯¹è±¡çš„æ•°æ® ---
                user_id = best_match.id
                distance = best_match.distance
                
                # è®¿é—® entity å±æ€§æ¥è·å– output_fields ä¸­çš„é¢å¤–æ•°æ®
                username = best_match.entity.get("user_name", f"æœªçŸ¥ç”¨æˆ·_{user_id[:6] if user_id else 'N/A'}")
                role = best_match.entity.get("role", UserRole.GUEST.value)
                
                if distance is not None and distance <= self.settings.VOICEPRINT_SIMILARITY_THRESHOLD:
                    confidence = 1.0 - (distance / self.settings.VOICEPRINT_SIMILARITY_THRESHOLD)
                    confidence = max(0.0, min(1.0, confidence))
                    confidence_percent = int(confidence * 100)

                    logger.info(f"è¯†åˆ«åˆ°è¯´è¯äºº: {username} (ID: {user_id}), è·ç¦»: {distance:.4f}, ç½®ä¿¡åº¦: {confidence_percent}%)")
                    return {
                        "user_id": user_id,
                        "username": username,
                        "role": role,
                        "confidence": confidence_percent,
                        "is_known": True
                    }
                else:
                    logger.info(f"æœªæ‰¾åˆ°è¶³å¤Ÿç›¸ä¼¼çš„å£°çº¹ (æœ€ä½³è·ç¦»: {distance:.4f}, é˜ˆå€¼: {self.settings.VOICEPRINT_SIMILARITY_THRESHOLD})ã€‚å°†è¿”å›æœªçŸ¥ç”¨æˆ·ã€‚")
                    return default_unknown_user
            else:
                logger.info("æœªåœ¨ Milvus ä¸­æ‰¾åˆ°åŒ¹é…çš„å£°çº¹ã€‚å°†è¿”å›æœªçŸ¥ç”¨æˆ·ã€‚")
                return default_unknown_user
        except Exception as e:
            logger.error(f"è¯†åˆ«è¯´è¯äººå¤±è´¥: {e}", exc_info=True)
            return {"user_id": "error", "username": "è¯†åˆ«é”™è¯¯", "role": UserRole.ERROR.value, "is_known": False, "confidence": 0}


    async def process_realtime_audio(self, audio_chunk: np.ndarray, sample_rate: int, meeting_id: str, stt_processor: SpeechToTextProcessor) -> List[Dict[str, Any]]:
        """
        [ä¼˜åŒ–] ä½¿ç”¨ Silero VAD é©±åŠ¨çš„å®æ—¶éŸ³é¢‘å¤„ç†ã€‚
        æ”¯æŒï¼š
        1. é™éŸ³è§¦å‘ (VAD_PAUSE_DURATION_S)
        2. è¶…æ—¶å¼ºåˆ¶è§¦å‘ (MAX_UTTERANCE_DURATION_S)
        """
        if not self._model_loaded:
            return []

        current_time = time.time()
        
        # åˆå§‹åŒ–ä¼šè®®çŠ¶æ€
        if meeting_id not in self.speech_buffer:
            self.speech_buffer[meeting_id] = []
            self.last_speech_timestamp[meeting_id] = current_time
            self.buffer_start_time[meeting_id] = current_time

        # --- 1. ä½¿ç”¨ Silero VAD è¿›è¡Œæ£€æµ‹ (å¦‚æœä¸å¯ç”¨åˆ™å›é€€) ---
        has_speech = False
        if self.vad_model:
            try:
                audio_tensor = torch.from_numpy(audio_chunk).float().to(self.device)
                # Silero VAD æœŸæœ› [batch, samples] æˆ– [samples]
                # è¿™é‡Œå‡è®¾ chunk å·²ç»è¶³å¤Ÿé•¿ (å¦‚ 32ms+)
                speech_prob = self.vad_model(audio_tensor, sample_rate).item()
                has_speech = speech_prob > self.settings.VAD_SPEECH_THRESHOLD
            except Exception as e:
                logger.error(f"Silero VAD å¤„ç†å¤±è´¥: {e}")
                has_speech = np.abs(audio_chunk).max() > self.settings.AUDIO_ENERGY_THRESHOLD
        else:
            # å›é€€åˆ°èƒ½é‡çº§åˆ«æ£€æµ‹
            has_speech = np.abs(audio_chunk).max() > self.settings.AUDIO_ENERGY_THRESHOLD

        # --- 2. æ›´æ–°ç¼“å†²åŒº logic ---
        if has_speech:
            self.speech_buffer[meeting_id].append(audio_chunk)
            self.last_speech_timestamp[meeting_id] = current_time
            # å¦‚æœæ˜¯åˆšå¼€å§‹è¯´è¯ï¼Œè®°å½•èµ·å§‹æ—¶é—´
            if len(self.speech_buffer[meeting_id]) == 1:
                self.buffer_start_time[meeting_id] = current_time
        
        # --- 3. æ£€æŸ¥è§¦å‘æ¡ä»¶ ---
        should_process = False
        reason = ""
        
        # æ¡ä»¶ A: æ£€æµ‹åˆ°é™éŸ³åœé¡¿
        pause_duration = current_time - self.last_speech_timestamp.get(meeting_id, current_time)
        if self.speech_buffer[meeting_id] and not has_speech and pause_duration > self.settings.VAD_PAUSE_DURATION_S:
            should_process = True
            reason = f"Silence pause ({pause_duration:.2f}s)"
            
        # æ¡ä»¶ B: è¯´è¯æ—¶é—´è¿‡é•¿ï¼Œå¼ºåˆ¶è½¬å½•ä¸€æ¬¡ (é˜²æ­¢é•¿æ—¶é—´ä¸è§¦å‘)
        utterance_duration = current_time - self.buffer_start_time.get(meeting_id, current_time)
        MAX_DURATION = 15.0 # ç¡¬ç¼–ç  15 ç§’å¼ºåˆ¶è§¦å‘ï¼Œä¹Ÿå¯æ”¾å…¥ settings
        if self.speech_buffer[meeting_id] and utterance_duration > MAX_DURATION:
            should_process = True
            reason = f"Max duration reach ({utterance_duration:.2f}s)"

        if should_process:
            logger.info(f"Triggering transcription for {meeting_id}. Reason: {reason}")
            complete_utterance = np.concatenate(self.speech_buffer[meeting_id])
            
            # æ¸…ç©ºç¼“å†²åŒºå’Œé‡ç½®æ—¶é—´æˆ³
            self.speech_buffer[meeting_id] = []
            self.buffer_start_time[meeting_id] = current_time # é‡ç½®èµ·å§‹æ—¶é—´
            
            try:
                return await self._process_utterance(complete_utterance, sample_rate, stt_processor)
            except Exception as e:
                logger.error(f"Error processing utterance for meeting {meeting_id}: {e}", exc_info=True)
        
        return []

    async def _process_utterance(self, audio_data: np.ndarray, sample_rate: int, stt_processor: SpeechToTextProcessor) -> List[Dict[str, Any]]:
        """
        å¤„ç†ä¸€ä¸ªå®Œæ•´çš„è¯­éŸ³ç‰‡æ®µï¼ˆä¸€å¥è¯ï¼‰ã€‚
        - è¿›è¡Œè¯´è¯äººåˆ†ç¦»
        - å¯¹æ¯ä¸ªè¯­éŸ³ç‰‡æ®µè¿›è¡ŒSTT
        - è¯†åˆ«æˆ–æ³¨å†Œè¯´è¯äºº
        """
        logger.info(f"Processing a complete utterance of {len(audio_data) / sample_rate:.2f}s.")
        
        if not self.embedding_model:
            logger.error("Embedding model not loaded, cannot process utterance.")
            return []

        waveform_tensor = torch.from_numpy(audio_data).float().to(self.device).unsqueeze(0)
        diarization = self.diarization_pipeline({"waveform": waveform_tensor, "sample_rate": sample_rate})

        results = []
        for segment, _, speaker_label in diarization.itertracks(yield_label=True):
            if segment.duration < self.settings.PYANNOTE_MIN_SPEECH_DURATION_S:
                continue # è·³è¿‡å¤ªçŸ­çš„ç‰‡æ®µ

            segment_audio = audio_data[int(segment.start * sample_rate):int(segment.end * sample_rate)]
            
            # 1. è¯†åˆ«è¯´è¯äºº
            identified_user = await self.identify_speaker(segment_audio, sample_rate)
            final_user_info = identified_user

            # 2. å¦‚æœæ˜¯æœªçŸ¥ç”¨æˆ·ï¼Œåˆ™è‡ªåŠ¨æ³¨å†Œ
            if not identified_user.get("is_known"):
                logger.info(f"Unknown speaker ({speaker_label}). Attempting to register new voiceprint.")
                new_user_id = str(uuid.uuid4())
                new_username = f"ç”¨æˆ·_{new_user_id[:6]}"
                new_role = UserRole.GUEST.value
                try:
                    reg_result = await self.register_voice(
                        audio_data=segment_audio,
                        sample_rate=sample_rate,
                        user_id=new_user_id,
                        username=new_username,
                        role=new_role
                    )
                    if reg_result.get("status") == "registered":
                        final_user_info = {"user_id": new_user_id, "username": new_username, "role": new_role, "is_known": True, "confidence": 100}
                        logger.info(f"New user '{new_username}' registered successfully.")
                    else:
                        logger.warning("Automatic registration failed.")
                except Exception as e:
                    logger.error(f"Error during automatic registration: {e}", exc_info=True)

            # 3. è¿›è¡Œè¯­éŸ³è½¬æ–‡å­—
            transcription_result = await stt_processor.transcribe_audio(segment_audio, sample_rate)
            transcribed_text = transcription_result.get("text", "")

            if transcribed_text:
                logger.info(f"User '{final_user_info.get('username')}' said: '{transcribed_text}'")
                results.append({
                    "audio": segment_audio,
                    "sample_rate": sample_rate,
                    "start_time": time.time() - (len(audio_data) / sample_rate) + segment.start,
                    "end_time": time.time() - (len(audio_data) / sample_rate) + segment.end,
                    "user_id": final_user_info.get("user_id"),
                    "username": final_user_info.get("username"),
                    "role": final_user_info.get("role"),
                    "is_new_user": not identified_user.get("is_known"),
                    "text": transcribed_text,
                    "confidence": transcription_result.get("confidence", 0.0),
                })
        
        return results

    async def process_audio_for_diarization(self, audio_file_path: str) -> List[Dict[str, Any]]:
        """
        å¯¹ç»™å®šçš„éŸ³é¢‘æ–‡ä»¶æ‰§è¡Œè¯´è¯äººåˆ†ç¦»ï¼Œå¹¶è¿”å›ç»“æ„åŒ–çš„ç»“æœã€‚
        """
        if not self.diarization_pipeline:
            raise RuntimeError("è¯´è¯äººåˆ†ç¦»æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•æ‰§è¡Œè¯´è¯äººåˆ†ç¦»ã€‚")

        if not os.path.exists(audio_file_path):
            logger.error(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_file_path}")
            raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_file_path}")

        logger.info(f"æ­£åœ¨å¯¹éŸ³é¢‘æ–‡ä»¶ '{audio_file_path}' è¿›è¡Œè¯´è¯äººåˆ†ç¦»...")
        try:
            diarization = await asyncio.to_thread(self.diarization_pipeline, audio_file_path)
            
            structured_results = []
            for segment, _, speaker_label in diarization.itertracks(yield_label=True):
                structured_results.append({
                    "start": segment.start,
                    "end": segment.end,
                    "speaker": speaker_label
                })
            logger.info(f"è¯´è¯äººåˆ†ç¦»å®Œæˆã€‚æ£€æµ‹åˆ° {len(set(r['speaker'] for r in structured_results))} ä¸ªè¯´è¯äººã€‚")
            return structured_results
        except Exception as e:
            logger.error(f"å¯¹éŸ³é¢‘æ–‡ä»¶ '{audio_file_path}' è¿›è¡Œè¯´è¯äººåˆ†ç¦»å¤±è´¥ï¼", exc_info=True)
            raise Exception(f"è¯´è¯äººåˆ†ç¦»å¤±è´¥: {e}") from e

    async def close(self):
        """
        å…³é—­ VoiceprintServiceï¼Œé‡Šæ”¾æ¨¡å‹èµ„æºã€‚
        """
        logger.info("Closing VoiceprintService...")
        self.diarization_pipeline = None
        self.embedding_model = None
        self.audio_processor = None
        self._model_loaded = False
        if torch.cuda.is_available():
            try:
                torch.cuda.empty_cache()
            except Exception as e:
                logger.warning(f"Failed to clear CUDA cache for VoiceprintService: {e}")
        logger.info("VoiceprintService closed.")
