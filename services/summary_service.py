# M:\meeting\services\summary_service.py

import logging
import os
import asyncio
import torch
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
from typing import Optional

# ******** å¼ºåˆ¶ Hugging Face Hub è¿›å…¥ç¦»çº¿æ¨¡å¼ ********
# è¿™ç¡®ä¿äº†æ¨¡å‹ä¼šä»æœ¬åœ°ç¼“å­˜åŠ è½½ï¼Œè€Œä¸æ˜¯å°è¯•ä»ç½‘ç»œä¸‹è½½ã€‚
os.environ["HF_HUB_OFFLINE"] = "1"
# ******************************************************

# ä»é¡¹ç›®é…ç½®ä¸­å¯¼å…¥è®¾ç½®
from config.settings import settings

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SummaryService:
    """
    è´Ÿè´£æ–‡æœ¬æ‘˜è¦çš„æœåŠ¡ç±»ã€‚
    """
    def __init__(self, settings_obj):
        """
        åˆå§‹åŒ– SummaryServiceã€‚
        Args:
            settings_obj: åº”ç”¨ç¨‹åºçš„è®¾ç½®å¯¹è±¡ï¼ŒåŒ…å« SUMMARY_MODEL_PATH ç­‰é…ç½®ã€‚
        """
        logger.info("æ­£åœ¨åˆå§‹åŒ– SummaryService...")
        self.settings = settings_obj
        self.model: Optional[AutoModelForSeq2SeqLM] = None
        self.tokenizer: Optional[AutoTokenizer] = None
        self.pipeline_instance = None # é¿å…ä¸ from transformers import pipeline å†²çª
        self.device = "cpu" # é»˜è®¤è®¾å¤‡
        self.model_loaded = False # åˆå§‹æ¨¡å‹åŠ è½½çŠ¶æ€

    async def load_model(self):
        """
        å¼‚æ­¥åŠ è½½æ–‡æœ¬æ‘˜è¦æ¨¡å‹ã€‚
        """
        if self.model_loaded:
            logger.info("æ‘˜è¦æ¨¡å‹å·²åŠ è½½ï¼Œè·³è¿‡é‡å¤åŠ è½½ã€‚")
            return

        try:
            # 1. æ£€æµ‹å¯ç”¨è®¾å¤‡ (CUDA æˆ– CPU)
            if torch.cuda.is_available():
                self.device = "cuda"
                logger.info("æ£€æµ‹åˆ° CUDAï¼Œå°†ä½¿ç”¨ GPU åŠ è½½æ‘˜è¦æ¨¡å‹ã€‚")
            else:
                self.device = "cpu"
                logger.info("æœªæ£€æµ‹åˆ° CUDAï¼Œå°†ä½¿ç”¨ CPU åŠ è½½æ‘˜è¦æ¨¡å‹ã€‚")

            # 2. å°è¯•åŠ è½½åˆ†è¯å™¨
            logger.info(f"å°è¯•ä»æœ¬åœ°è·¯å¾„åŠ è½½åˆ†è¯å™¨: '{self.settings.SUMMARY_MODEL_PATH}'...")
            if not os.path.exists(self.settings.SUMMARY_MODEL_PATH):
                logger.critical(f"âŒ é”™è¯¯ï¼šæ‘˜è¦æ¨¡å‹æœ¬åœ°è·¯å¾„ä¸å­˜åœ¨: '{self.settings.SUMMARY_MODEL_PATH}'ã€‚è¯·ç¡®ä¿æ–‡ä»¶å·²æ­£ç¡®æ”¾ç½®ã€‚")
                raise FileNotFoundError(f"æ‘˜è¦æ¨¡å‹æœ¬åœ°è·¯å¾„ä¸å­˜åœ¨: {self.settings.SUMMARY_MODEL_PATH}")

            try:
                self.tokenizer = await asyncio.to_thread(
                    AutoTokenizer.from_pretrained,
                    self.settings.SUMMARY_MODEL_PATH, # ç›´æ¥ä½¿ç”¨æœ¬åœ°è·¯å¾„
                    local_files_only=True # æ˜ç¡®æŒ‡å®šåªä»æœ¬åœ°æ–‡ä»¶åŠ è½½
                )
                logger.info("âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸã€‚")
            except Exception as e:
                logger.critical(f"âŒ é”™è¯¯ï¼šåˆ†è¯å™¨åŠ è½½å¤±è´¥ï¼è¯·æ£€æŸ¥ '{self.settings.SUMMARY_MODEL_PATH}' ä¸­çš„åˆ†è¯å™¨æ–‡ä»¶æ˜¯å¦å®Œæ•´ä¸”å…¼å®¹ã€‚")
                logger.critical(f"é”™è¯¯ä¿¡æ¯ (åˆ†è¯å™¨): {e}", exc_info=True)
                raise RuntimeError(f"æ— æ³•åŠ è½½åˆ†è¯å™¨: {e}") from e

            # 3. å°è¯•åŠ è½½æ¨¡å‹
            logger.info(f"å°è¯•ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹: '{self.settings.SUMMARY_MODEL_PATH}' (è®¾å¤‡: {self.device})...")
            try:
                self.model = await asyncio.to_thread(
                    AutoModelForSeq2SeqLM.from_pretrained,
                    self.settings.SUMMARY_MODEL_PATH, # ç›´æ¥ä½¿ç”¨æœ¬åœ°è·¯å¾„
                    local_files_only=True # æ˜ç¡®æŒ‡å®šåªä»æœ¬åœ°æ–‡ä»¶åŠ è½½
                )
                self.model.to(self.device) # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
                logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸã€‚")
            except Exception as e:
                logger.critical(f"âŒ é”™è¯¯ï¼šæ¨¡å‹åŠ è½½å¤±è´¥ï¼è¯·æ£€æŸ¥ '{self.settings.SUMMARY_MODEL_PATH}' ä¸­çš„æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´ä¸”å…¼å®¹ã€‚")
                logger.critical(f"é”™è¯¯ä¿¡æ¯ (æ¨¡å‹): {e}", exc_info=True)
                raise RuntimeError(f"æ— æ³•åŠ è½½æ¨¡å‹: {e}") from e

            # 4. åˆ›å»º Hugging Face pipeline
            logger.info("æ­£åœ¨åˆ›å»º Hugging Face pipeline...")
            try:
                self.pipeline_instance = await asyncio.to_thread(
                    pipeline,
                    "summarization", # ä»»åŠ¡ç±»å‹
                    model=self.model,
                    tokenizer=self.tokenizer,
                    device=0 if self.device == "cuda" else -1 # 0 for GPU, -1 for CPU
                )
                logger.info("âœ… Hugging Face pipeline åˆ›å»ºæˆåŠŸã€‚")
            except Exception as e:
                logger.critical(f"âŒ é”™è¯¯ï¼šåˆ›å»º Hugging Face pipeline å¤±è´¥ï¼")
                logger.critical(f"é”™è¯¯ä¿¡æ¯ (pipeline): {e}", exc_info=True)
                raise RuntimeError(f"æ— æ³•åˆ›å»º pipeline: {e}") from e

            self.model_loaded = True # This line is only reached if all above steps succeed
            logger.info(f"ğŸ‰ æ‘˜è¦æ¨¡å‹ '{self.settings.SUMMARY_MODEL_HUB_NAME}' å·²æˆåŠŸä»æœ¬åœ°è·¯å¾„åŠ è½½åˆ° {self.device.upper()}ã€‚")

        except RuntimeError as e: # æ•è·å†…éƒ¨æŠ›å‡ºçš„ RuntimeError
            self.model_loaded = False # ç¡®ä¿åœ¨å¤±è´¥æ—¶è®¾ç½®ä¸º False
            logger.critical(f"æ‘˜è¦æ¨¡å‹åŠ è½½æµç¨‹ä¸­æ–­: {e}")
            raise # é‡æ–°æŠ›å‡ºï¼Œè®© lifespan æ•è·
        except Exception as e: # æ•è·å…¶ä»–æ„å¤–é”™è¯¯
            self.model_loaded = False # ç¡®ä¿åœ¨å¤±è´¥æ—¶è®¾ç½®ä¸º False
            logger.critical(f"âŒ é”™è¯¯ï¼šæ‘˜è¦æ¨¡å‹åŠ è½½è¿‡ç¨‹ä¸­å‘ç”Ÿæ„å¤–é”™è¯¯ï¼")
            logger.critical(f"é”™è¯¯ä¿¡æ¯: {e}", exc_info=True)
            raise RuntimeError(f"æ‘˜è¦æ¨¡å‹åŠ è½½å¤±è´¥: {e}") from e
        finally:
            # æ— è®ºæˆåŠŸæˆ–å¤±è´¥ï¼Œéƒ½åœ¨è¿™é‡Œè®°å½•æœ€ç»ˆçŠ¶æ€
            logger.info(f"SummaryService.load_model() æ–¹æ³•ç»“æŸã€‚æœ€ç»ˆ model_loaded çŠ¶æ€: {self.model_loaded}")


    def is_model_loaded(self) -> bool:
        """
        æ£€æŸ¥æ‘˜è¦æ¨¡å‹æ˜¯å¦å·²åŠ è½½ã€‚
        """
        return self.model_loaded

    async def generate_summary(self, text: str, max_length: int = 150, min_length: int = 30, num_beams: int = 4) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬æ‘˜è¦ã€‚
        Args:
            text (str): è¦æ‘˜è¦çš„æ–‡æœ¬ã€‚
            max_length (int): ç”Ÿæˆæ‘˜è¦çš„æœ€å¤§é•¿åº¦ã€‚
            min_length (int): ç”Ÿæˆæ‘˜è¦çš„æœ€å°é•¿åº¦ã€‚
            num_beams (int): Beam search çš„æŸå®½ã€‚
        Returns:
            str: ç”Ÿæˆçš„æ‘˜è¦æ–‡æœ¬ã€‚
        """
        if not self.model_loaded or self.pipeline_instance is None:
            logger.error("æ‘˜è¦æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦ã€‚")
            return "é”™è¯¯ï¼šæ‘˜è¦æœåŠ¡æœªå‡†å¤‡å¥½ã€‚"

        try:
            # å¼‚æ­¥æ‰§è¡Œæ‘˜è¦ç”Ÿæˆ
            summary_list = await asyncio.to_thread(
                self.pipeline_instance, # ä½¿ç”¨æ›´ååçš„ pipeline_instance
                text,
                max_length=max_length,
                min_length=min_length,
                num_beams=num_beams,
                do_sample=False # é€šå¸¸æ‘˜è¦æ˜¯ç¡®å®šæ€§çš„
            )
            summary_text = summary_list[0]['summary_text']
            logger.info(f"æˆåŠŸç”Ÿæˆæ‘˜è¦ (é•¿åº¦: {len(summary_text)})ã€‚")
            return summary_text
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}", exc_info=True)
            return f"ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}"

    async def close(self):
        """
        å…³é—­ SummaryServiceï¼Œé‡Šæ”¾æ¨¡å‹èµ„æºã€‚
        """
        logger.info("Closing SummaryService...")
        self.model = None
        self.tokenizer = None
        self.pipeline_instance = None
        self.model_loaded = False
        if torch.cuda.is_available():
            try:
                torch.cuda.empty_cache()
                logger.info("CUDA cache cleared for SummaryService.")
            except Exception as e:
                logger.warning(f"Failed to clear CUDA cache for SummaryService: {e}")
        logger.info("SummaryService closed.")

